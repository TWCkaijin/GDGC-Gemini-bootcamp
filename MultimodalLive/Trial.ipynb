{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXnEutuDQa9c"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Getting Started with the Multimodal Live API using Gen AI SDK\n",
        "\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/multimodal-live-api/intro_multimodal_live_api_genai_sdk.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fmultimodal-live-api%2Fintro_multimodal_live_api_genai_sdk.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/multimodal-live-api/intro_multimodal_live_api_genai_sdk.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/multimodal-live-api/intro_multimodal_live_api_genai_sdk.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overview\n",
        "Multimodal Live API 提供低延遲雙向 Gemini 互動。輸入資料可以為文字、聲音或影片，輸出資料可以是文字或是聲音。這次的教程會是一個簡單針對 Vertex AI 中 Multimodal Live API 的範例。\n",
        "\n",
        "本次的範例包含：\n",
        "- 文字對文字生成\n",
        "- 文字對語音生成\n",
        "- 文字對語音對話\n",
        "- 函數呼叫\n",
        "- 程式碼執行\n",
        "- Google搜尋\n",
        "\n",
        "在 [Multimodal Live API](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/multimodal-live) 頁面看更多資訊"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPiTOAHURvTM"
      },
      "source": [
        "## Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHRZUpfWSEpp"
      },
      "source": [
        "### Install Google Gen AI SDK for Python\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "首先會是安裝所需要的套件，在 notebook 中前面打%的指令會在 terminal 中執行 \n",
        "這次安裝的套件會是 google-genai ，此套件將會允許我們透過 Python 調用 Gemini "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sG3_LKsWSD3A"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlMVjiAWSMNX"
      },
      "source": [
        "### Authenticate your notebook environment\n",
        "我們將在 colab 上運行，須先驗證環境"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type: \"string\"}\n",
        "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
        "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "\n",
        "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n",
        "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ef0zVX-X9Bg"
      },
      "source": [
        "### Import libraries\n",
        "\n",
        "接下來會調用需要的套件\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xBCH3hnAX9Bh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from IPython.display import Audio, Markdown, display\n",
        "from google import genai\n",
        "from google.genai.types import (\n",
        "    FunctionDeclaration,\n",
        "    GoogleSearch,\n",
        "    LiveConnectConfig,\n",
        "    PrebuiltVoiceConfig,\n",
        "    SpeechConfig,\n",
        "    Tool,\n",
        "    ToolCodeExecution,\n",
        "    VoiceConfig,\n",
        ")\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set Google Cloud project information and create client\n",
        "\n",
        "\n",
        "首先在 [Google Cloud Console](https://console.cloud.google.com/welcome) 上新增一個專案，接下來[開啟 Vertex AI API ](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)，接下來將下面的`[your-project-id]`改成你的Project ID，這一步將會設定你所使用的專案名稱。下一步將會設定用戶端使用你所設定的專案。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Nqwi-5ufWp_B"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # \n",
        "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
        "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "\n",
        "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "T-tiytzQE0uM"
      },
      "outputs": [],
      "source": [
        "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Use the Gemini 2.0 Flash model\n",
        "\n",
        "本次使用的模型為 Gemini 2.0 flash，可以視使用情況做更改"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-coEslfWPrxo"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"gemini-2.0-flash-exp\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b51c5ced31f7"
      },
      "source": [
        "## Use the Multimodal Live API\n",
        "\n",
        "Multimodal Live API is a stateful API that uses [WebSockets](https://en.wikipedia.org/wiki/WebSocket). This section shows some basic examples of how to use Multimodal Live API for text-to-text and text-to-audio generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Multimodal Live API 使用 [WebSockets](https://en.wikipedia.org/wiki/WebSocket) 來建立雙向連接。其跟一般http request的差別為她只需要做一次的連接就可以保持雙向連接，不需要重複發request。這個區塊會包含文字對文字還有文字對語音的範例"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Example 1**: Text-to-text generation\n",
        "\n",
        "本範例為發送訊息，並獲得文字回覆\n",
        "\n",
        "**Notes**\n",
        " - `Session`代表一段WebSocket連接\n",
        " - 當連接成功後，可以選擇傳送`語音` `文字`或`影片`，回傳的資料也可以選擇為`音檔` `文字`或`函數`\n",
        " - `response_modalities` 可以設定回傳得資料為 `TEXT` 或 `AUDIO`\n",
        " - 將 `end_of_turn` 設定為 `True` 來根據現有資料回覆，否則將會等待更多資料\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "NbJZzc7CIha5"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Input:** Hello? Gemini are you there?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**Response >** Yes, I'm here. What would you like to talk about today?\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "config = LiveConnectConfig(response_modalities=[\"TEXT\"])\n",
        "\n",
        "async with client.aio.live.connect(\n",
        "    model=MODEL_ID,\n",
        "    config=config,\n",
        ") as session:\n",
        "    text_input = \"Hello? Gemini are you there?\"\n",
        "    display(Markdown(f\"**Input:** {text_input}\"))\n",
        "\n",
        "    await session.send(input=text_input, end_of_turn=True)\n",
        "\n",
        "    response = []\n",
        "\n",
        "    async for message in session.receive():\n",
        "        if message.text:\n",
        "            response.append(message.text)\n",
        "\n",
        "    display(Markdown(f\"**Response >** {''.join(response)}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cG3346aA9sRR"
      },
      "source": [
        "### **Example 2**: Text-to-audio generation\n",
        "\n",
        "本範例為發送訊息，並獲得語音回覆\n",
        "\n",
        "**Notes**\n",
        "- Multimodal Live API 支援以下幾種人聲:\n",
        "  - Puck\n",
        "  - Charon\n",
        "  - Kore\n",
        "  - Fenrir\n",
        "  - Aoede\n",
        "- 設定 `speech_config` 裡的 `voice_name` 來指定人聲"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iz3OkQ-a51QM"
      },
      "outputs": [],
      "source": [
        "config = LiveConnectConfig(\n",
        "    response_modalities=[\"AUDIO\"],\n",
        "    speech_config=SpeechConfig(\n",
        "        voice_config=VoiceConfig(\n",
        "            prebuilt_voice_config=PrebuiltVoiceConfig(\n",
        "                voice_name=\"Aoede\",\n",
        "            )\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "\n",
        "async with client.aio.live.connect(\n",
        "    model=MODEL_ID,\n",
        "    config=config,\n",
        ") as session:\n",
        "    text_input = \"Hello? Gemini are you there?\"\n",
        "    display(Markdown(f\"**Input:** {text_input}\"))\n",
        "\n",
        "    await session.send(input=text_input, end_of_turn=True)\n",
        "\n",
        "    audio_data = []\n",
        "    async for message in session.receive():\n",
        "        if message.server_content.model_turn:\n",
        "            for part in message.server_content.model_turn.parts:\n",
        "                if part.inline_data:\n",
        "                    audio_data.append(\n",
        "                        np.frombuffer(part.inline_data.data, dtype=np.int16)\n",
        "                    )\n",
        "\n",
        "    if audio_data:\n",
        "        display(Audio(np.concatenate(audio_data), rate=24000, autoplay=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOBlWf566HOx"
      },
      "source": [
        "### **Example 3**: Text-to-audio conversation\n",
        "\n",
        "**Step 1**: 這裡示範了透過 API 傳送文字並獲得語音回覆的對話函數\n",
        "\n",
        "**Notes**\n",
        "- 模型會記錄當下 session 的互動，但一旦結束 session 聊天記錄將會被抹除，無法透過API找回"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "bhY0P0qpRP5y"
      },
      "outputs": [],
      "source": [
        "config = LiveConnectConfig(response_modalities=[\"AUDIO\"])\n",
        "\n",
        "\n",
        "async def main() -> None:\n",
        "    async with client.aio.live.connect(model=MODEL_ID, config=config) as session:\n",
        "\n",
        "        async def send() -> bool:\n",
        "            text_input = input(\"Input > \")\n",
        "            if text_input.lower() in (\"q\", \"quit\", \"exit\"):\n",
        "                return False\n",
        "            await session.send(input=text_input, end_of_turn=True)\n",
        "            return True\n",
        "\n",
        "        async def receive() -> None:\n",
        "\n",
        "            audio_data = []\n",
        "\n",
        "            async for message in session.receive():\n",
        "                if message.server_content.model_turn:\n",
        "                    for part in message.server_content.model_turn.parts:\n",
        "                        if part.inline_data:\n",
        "                            audio_data.append(\n",
        "                                np.frombuffer(part.inline_data.data, dtype=np.int16)\n",
        "                            )\n",
        "\n",
        "                if message.server_content.turn_complete:\n",
        "                    display(Markdown(\"**Response >**\"))\n",
        "                    display(\n",
        "                        Audio(np.concatenate(audio_data), rate=24000, autoplay=True)\n",
        "                    )\n",
        "                    break\n",
        "\n",
        "            return\n",
        "\n",
        "        while True:\n",
        "            if not await send():\n",
        "                break\n",
        "            await receive()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94IeUUb3e90M"
      },
      "source": [
        "**Step 2** 運行此對話，輸入你的指令，或輸入`q`,`quit`或`exit`來退出。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UvgUDIYJqfw"
      },
      "outputs": [],
      "source": [
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "907da7836dcf"
      },
      "source": [
        "### **Example 4**: Function calling\n",
        "\n",
        "你可以透過 function calling 來組成函數的描述，再傳送該描述到模型中。模型會在描述成立的時候回傳函數呼叫指令，並且將參數傳入\n",
        "\n",
        "**Notes**:\n",
        "- 所有函數都需要在 session 開始的時候透過 tool definition 做定義\n",
        "- 目前 API 只支援一種工具"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Input:** Get the current weather in Santa Clara, San Jose and Mountain View"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**FunctionCall >** id=None args={'location': 'Santa Clara'} name='get_current_weather'"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**FunctionCall >** id=None args={'location': 'San Jose'} name='get_current_weather'"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**FunctionCall >** id=None args={'location': 'Mountain View'} name='get_current_weather'"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "get_current_weather = FunctionDeclaration(\n",
        "    name=\"get_current_weather\",\n",
        "    description=\"Get current weather in the given location\",\n",
        "    parameters={\n",
        "        \"type\": \"OBJECT\",\n",
        "        \"properties\": {\n",
        "            \"location\": {\n",
        "                \"type\": \"STRING\",\n",
        "            },\n",
        "        },\n",
        "    },\n",
        ")\n",
        "\n",
        "config = LiveConnectConfig(\n",
        "    response_modalities=[\"TEXT\"],\n",
        "    tools=[Tool(function_declarations=[get_current_weather])],\n",
        ")\n",
        "\n",
        "async with client.aio.live.connect(\n",
        "    model=MODEL_ID,\n",
        "    config=config,\n",
        ") as session:\n",
        "    text_input = \"Get the current weather in Santa Clara, San Jose and Mountain View\"\n",
        "    display(Markdown(f\"**Input:** {text_input}\"))\n",
        "\n",
        "    await session.send(input=text_input, end_of_turn=True)\n",
        "\n",
        "    async for message in session.receive():\n",
        "        if message.tool_call:\n",
        "            for function_call in message.tool_call.function_calls:\n",
        "                display(Markdown(f\"**FunctionCall >** {str(function_call)}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23cb7ab89311"
      },
      "source": [
        "### **Example 5**: Code Execution\n",
        "\n",
        " 你可以使用 API 的 code exectution 能力來生成並執行 Python 程式\n",
        "\n",
        " 在這個範例中，我們在 `Tool` 裡面傳入 `code_execution`，並在 session 開始時初始化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "631b0c59c516"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Input:** Write code to calculate the 15th fibonacci number then find the nearest palindrome to it"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**Response >** Okay, I understand. Here's the plan:\n",
              "\n",
              "1.  **Calculate the 15th Fibonacci number:** I'll use a simple iterative approach for this.\n",
              "2.  **Find the nearest palindrome:** Once I have the Fibonacci number, I'll create a function to find the nearest palindrome. This will involve checking both smaller and larger numbers to see which palindrome is closest.\n",
              "\n",
              "Here's the Python code:\n",
              "\n",
              "```python\n",
              "def fibonacci(n):\n",
              "  \"\"\"Calculates the nth Fibonacci number.\"\"\"\n",
              "  a, b = 0, 1\n",
              "  for _ in range(n):\n",
              "    a, b = b, a + b\n",
              "  return a\n",
              "\n",
              "def nearest_palindrome(num):\n",
              "  \"\"\"Finds the nearest palindrome to a given number.\"\"\"\n",
              "  num_str = str(num)\n",
              "  length = len(num_str)\n",
              "\n",
              "  # Helper function to generate palindrome\n",
              "  def generate_palindrome(number_str, length, even):\n",
              "    if even:\n",
              "      return number_str + number_str[::-1]\n",
              "    else:\n",
              "      return number_str + number_str[:-1][::-1]\n",
              "  \n",
              "  # Create a palindrome from the first half of the number\n",
              "  first_half = num_str[:(length + 1) // 2]\n",
              "  \n",
              "  # Generate even and odd length palindromes based on the first half\n",
              "  even_palindrome  = int(generate_palindrome(first_half, length, length % 2 == 0))\n",
              "\n",
              "  #Generate smaller and larger values of the first half\n",
              "  smaller_half = str(int(first_half) -1)\n",
              "  larger_half = str(int(first_half) + 1)\n",
              "  \n",
              "  #Account for the edge cases\n",
              "  if smaller_half == '-1':\n",
              "    smaller_palindrome = 0\n",
              "  else:\n",
              "    smaller_palindrome  = int(generate_palindrome(smaller_half, length, length % 2 == 0))\n",
              "\n",
              "  larger_palindrome  = int(generate_palindrome(larger_half, length, length % 2 == 0))\n",
              "\n",
              "  # Find the differences\n",
              "  diff_smaller = abs(num - smaller_palindrome)\n",
              "  diff_larger = abs(num - larger_palindrome)\n",
              "  diff_even = abs(num - even_palindrome)\n",
              "\n",
              "  # Return the closest palindrome\n",
              "  if diff_smaller <= diff_larger and diff_smaller <= diff_even:\n",
              "    return smaller_palindrome\n",
              "  elif diff_larger <= diff_smaller and diff_larger <= diff_even:\n",
              "    return larger_palindrome\n",
              "  else:\n",
              "    return even_palindrome\n",
              "  \n",
              "# Calculate the 15th Fibonacci number\n",
              "fib_15 = fibonacci(15)\n",
              "print(f\"The 15th Fibonacci number is: {fib_15}\")\n",
              "\n",
              "# Find the nearest palindrome\n",
              "nearest_pal = nearest_palindrome(fib_15)\n",
              "print(f\"The nearest palindrome to {fib_15} is: {nearest_pal}\")\n",
              "```\n",
              "\n",
              "This code first calculates the 15th Fibonacci number, which is 610. Then, the `nearest_palindrome` function generates potential palindromes smaller, equal and larger than the number and finds the closest one. Finally, the code prints the results.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "config = LiveConnectConfig(\n",
        "    response_modalities=[\"TEXT\"],\n",
        "    tools=[Tool(code_execution=ToolCodeExecution())],\n",
        ")\n",
        "\n",
        "async with client.aio.live.connect(\n",
        "    model=MODEL_ID,\n",
        "    config=config,\n",
        ") as session:\n",
        "    text_input = \"Write code to calculate the 15th fibonacci number then find the nearest palindrome to it\"\n",
        "    display(Markdown(f\"**Input:** {text_input}\"))\n",
        "\n",
        "    await session.send(input=text_input, end_of_turn=True)\n",
        "\n",
        "    response = []\n",
        "\n",
        "    async for message in session.receive():\n",
        "        if message.text:\n",
        "            response.append(message.text)\n",
        "        if message.server_content.model_turn.parts:\n",
        "            for part in message.server_content.model_turn.parts:\n",
        "                if part.executable_code:\n",
        "                    display(\n",
        "                        Markdown(\n",
        "                            f\"\"\"\n",
        "**Executable code:**\n",
        "```py\n",
        "{part.executable_code.code}\n",
        "```\n",
        "\"\"\"\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "    display(Markdown(f\"**Response >** {''.join(response)}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73660342318d"
      },
      "source": [
        "### **Example 6**: Google Search\n",
        "\n",
        "`google_search` 工具可以利用 Google Search 功能，我們可以用新到不可能在訓練集的問題做測試。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "e64fc3a94d49"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Input:** Tell me about the largest earthquake in California the week of Dec 5 2024?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**Response >** The largest earthquake in California during the week of December 5, 2024, occurred on December 5th. It had a magnitude of 7.0 and struck off the coast of Northern California, about 54 miles southwest of Eureka, in Humboldt County. A tsunami warning was issued for parts of California and Oregon but was later canceled. There were reports of non-structural impacts and some structural damage in Humboldt County.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "config = LiveConnectConfig(\n",
        "    response_modalities=[\"TEXT\"],\n",
        "    tools=[Tool(google_search=GoogleSearch())],\n",
        ")\n",
        "\n",
        "async with client.aio.live.connect(\n",
        "    model=MODEL_ID,\n",
        "    config=config,\n",
        ") as session:\n",
        "    text_input = (\n",
        "        \"Tell me about the largest earthquake in California the week of Dec 5 2024?\"\n",
        "    )\n",
        "    display(Markdown(f\"**Input:** {text_input}\"))\n",
        "\n",
        "    await session.send(input=text_input, end_of_turn=True)\n",
        "\n",
        "    response = []\n",
        "\n",
        "    async for message in session.receive():\n",
        "        if message.text:\n",
        "            response.append(message.text)\n",
        "\n",
        "    display(Markdown(f\"**Response >** {''.join(response)}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usjiqTDXfk_6"
      },
      "source": [
        "## What's next\n",
        "\n",
        "- Learn how to [build a web application that enables you to use your voice and camera to talk to Gemini 2.0 through the Multimodal Live API.](https://github.com/GoogleCloudPlatform/generative-ai/tree/main/gemini/multimodal-live-api/websocket-demo-app)\n",
        "- See the [Multimodal Live API reference docs](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/multimodal-live).\n",
        "- See the [Google Gen AI SDK reference docs](https://googleapis.github.io/python-genai/).\n",
        "- Explore other notebooks in the [Google Cloud Generative AI GitHub repository](https://github.com/GoogleCloudPlatform/generative-ai)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What We're Actually Doing Next\n",
        " - Try out app [built on Gemini Multimodal Live API](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/multimodal-live-api/websocket-demo-app/README.md)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "intro_multimodal_live_api_genai_sdk.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
