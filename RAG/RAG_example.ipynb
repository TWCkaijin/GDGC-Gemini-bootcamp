{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijGzTHJJUCPY"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDsTUvKjwHBW"
      },
      "source": [
        "# RAG 基本實作\n",
        "<table>\n",
        "<td style=\"text-align: center\">\n",
        "  <a href=\"https://colab.research.google.com/github/TWCkaijin/GDGC-Gemini-bootcamp/blob/main/RAG/RAG_example.ipynb\">\n",
        "    <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
        "  </a>\n",
        "</td>        \n",
        "<td style=\"text-align: center\">\n",
        "  <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FTWCkaijin%2FGDGC-Gemini-bootcamp%2Fmain%2FRAG%2fRAG_example.ipynb\">\n",
        "    <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "  </a>\n",
        "</td>\n",
        "\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsjCZ1v9rP7s"
      },
      "source": [
        "| | |\n",
        "|-|-|\n",
        "|原作者 | [Lavi Nigam](https://github.com/lavinigam-gcp) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0KqoJXJHzkT"
      },
      "source": [
        "DIY version of RAG [**building_DIY_multimodal_qa_system_with_mRAG.ipynb**](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/qa-ops/building_DIY_multimodal_qa_system_with_mRAG.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VK1Q5ZYdVL4Y"
      },
      "source": [
        "## 概述\n",
        "檢索增強生成（Retrieval Augmented Generation, RAG）已成為讓大型語言模型（LLMs）存取外部數據的重要範式，同時也作為一種機制，能有效減少幻覺現象（hallucinations）。\n",
        "\n",
        "在此筆記本中，您將學習如何進行多模態 RAG，透過結合文字與圖片的財務文件進行問答。\n",
        "\n",
        "## Gemini\n",
        "- Gemini 是 Google DeepMind 開發的一系列生成式 AI 模型，專為多模態應用設計。\n",
        "- Gemini API 提供對 Gemini 1.0 Pro Vision 和 Gemini 1.0 Pro 模型的存取。\n",
        "\n",
        "## 比較文字型與多模態 RAG\n",
        "多模態 RAG 相較於文字型 RAG 具有以下幾項優勢：\n",
        "\n",
        "1. 增強的知識存取能力：\n",
        "多模態 RAG 能處理文字與視覺資訊，為大型語言模型提供更豐富、更全面的知識基礎。\n",
        "\n",
        "2. 改進的推理能力：\n",
        "藉由整合視覺線索，多模態 RAG 能在不同數據模態之間進行更明智的推斷。\n",
        "\n",
        "此筆記本將教您如何在 Vertex AI 中，結合 Gemini API、文字嵌入 和 多模態嵌入，構建文件搜尋引擎。\n",
        "\n",
        "透過實際操作範例，您將學會如何為文件來源建構一個多媒體豐富的中繼資料庫，實現跨多元資訊流的搜尋、比較與推理功能。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQT500QqVPIb"
      },
      "source": [
        "## 單元目標\n",
        "本筆記本逐步提供建構文件搜尋引擎的指南，透過多模態檢索增強生成（RAG）實現以下功能：\n",
        "\n",
        "- 提取並儲存包含文字的文件中繼資料(metadata)，並為文件生成嵌入向量\n",
        "- 使用文字查詢搜尋中繼資料(metadata)，並向LLM提供檢索結果並回答問題"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnpYxfesh2rI"
      },
      "source": [
        "## 工具:\n",
        "請注意，專案必須要啟動以下GCP API服務:\n",
        "- Vertex AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXJpXzKrh2rJ"
      },
      "source": [
        "## 基本設定"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5afkyDMSBW5"
      },
      "source": [
        "### 下載安裝 Vertex AI SDK for Python 及其他相依套件"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kc4WxYmLSBW5",
        "outputId": "ced9494a-bd24-4097-bacd-78ed6c556b94"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --user google-cloud-aiplatform pymupdf rich colorama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5Xep4W9lq-Z"
      },
      "source": [
        "### 重啟以確保套件成功載入"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRvKdaPDTznN",
        "outputId": "f9688ae8-3c02-4151-e9b2-72e396d79b09"
      },
      "outputs": [],
      "source": [
        "# Restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbmM4z7FOBpM"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtsU9Bw9h2rL"
      },
      "source": [
        "### 驗證Colab 環境(僅Colab需要)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpYEyLsOh2rL"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# Additional authentication is required for Google Colab\n",
        "if \"google.colab\" in sys.modules:\n",
        "    # Authenticate user to Google Cloud\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1vKZZoEh2rL"
      },
      "source": [
        "### 初始化 GCP 專案設定\n",
        "請注意，專案必須要啟動以下API服務:\n",
        "- Vertex AI API\n",
        "\n",
        "https://console.cloud.google.com</br>\n",
        "\n",
        "以下輸入您的專案ID以及欲使用的伺服器區域。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJqZ76rJh2rM",
        "outputId": "99cf4f3d-60b2-4a68-94ff-432bfab19742"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "PROJECT_ID = \"[YOUR-PROJECT-ID]\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "# if not running on Colab, try to get the PROJECT_ID automatically\n",
        "if \"google.colab\" not in sys.modules:\n",
        "    import subprocess\n",
        "\n",
        "    PROJECT_ID = subprocess.check_output(\n",
        "        [\"gcloud\", \"config\", \"get-value\", \"project\"], text=True\n",
        "    ).strip()\n",
        "\n",
        "print(f\"Your project ID is: {PROJECT_ID}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umWOVxrBdRR6"
      },
      "source": [
        "### 初始化 Vertex AI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D48gUW5-h2rM"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# Initialize Vertex AI\n",
        "import vertexai\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuQwwRiniVFG"
      },
      "source": [
        "### 導入函式庫"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtMowvm-yQ97"
      },
      "outputs": [],
      "source": [
        "from vertexai.generative_models import GenerationConfig, GenerativeModel, Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-TX_R_xh2rM"
      },
      "source": [
        "### 定義及載入 Gemini 1.5 Pro 和 Gemini 1.5 Flash 模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvMwSRJJh2rM"
      },
      "outputs": [],
      "source": [
        "text_model = GenerativeModel(\"gemini-1.5-pro\")\n",
        "multimodal_model = GenerativeModel(\"gemini-1.5-pro\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lCfREXK5SWD"
      },
      "source": [
        "### 下載示範用檔案以及額外外掛套件\n",
        "\n",
        "原作者:\n",
        "You can view the code for the utils here: (`intro_multimodal_rag_utils.py`) directly on [GitHub](https://storage.googleapis.com/github-repo/rag/intro_multimodal_rag/intro_multimodal_rag_old_version/intro_multimodal_rag_utils.py)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwbL89zcY39N",
        "outputId": "4e21d792-c9af-4555-b5cd-136dcbdef63c"
      },
      "outputs": [],
      "source": [
        "# download documents and images used in this notebook\n",
        "!gsutil -m rsync -r gs://github-repo/rag/intro_multimodal_rag/intro_multimodal_rag_old_version .\n",
        "!mkdir pdf\n",
        "!curl -o ./pdf/example.pdf https://raw.githubusercontent.com/TWCkaijin/GDGC-Gemini-bootcamp/main/RAG/examplefile.pdf\n",
        "print(\"Download completed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps1G-cCfpibN"
      },
      "source": [
        "## 建立包含文字metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqLsy3iZ5t-R"
      },
      "source": [
        "### 範例資料內容\n",
        "\n",
        "在此筆記本中，我們將使用的原始數據為一篇「虛構角色的生平故事」\n",
        "\n",
        "單一、獨立且無出現在網路上的內容，適合進行多模態檢索增強生成（RAG）的實驗與學習。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvt0sus5KSNX"
      },
      "source": [
        "\n",
        "### 匯入輔助函數以建立metadata\n",
        "在構建多模態 RAG 系統之前，需先準備文件中所有文字與圖片的metadata。為了便於引用與參考，metadata應包含關鍵元素，例如頁碼、檔案名稱等。\n",
        "\n",
        "接下來，您將從這些metadata中生成嵌入向量，這些嵌入向量是執行相似性搜尋時的必要條件。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3wo2jv2rP7v"
      },
      "outputs": [],
      "source": [
        "from intro_multimodal_rag_utils import get_document_metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BOAkYN0KlSL"
      },
      "source": [
        "### 從文件中提取並儲存文字和圖片的metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9hBPPWs5CMd"
      },
      "source": [
        "我們剛剛匯入了一個名為 ```get_document_metadata()``` 的函數。這個函數會從文件中提取文字和圖片的中繼資料，並返回兩個資料框，分別為 ```text_metadata``` 和 ```image_metadata``` 。如果你想了解更多有關 ```get_document_metadata()``` 函數如何使用 Gemini 和嵌入模型實現的細節，你可以直接查看[source code](https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/use-cases/retrieval-augmented-generation/utils/intro_multimodal_rag_utils.py)。\n",
        "\n",
        "提取並儲存文字和圖片metadata的原因在於，僅使用其中一個資料類型不足以得出相關的答案。例如，相關答案可能以視覺形式存在於文件中，但文字型 RAG 無法考慮到視覺圖像。你稍後會在這本筆記本中探索這個範例。\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnKru0sBh2rN"
      },
      "source": [
        "在下一步，我們將使用這個函數來提取並儲存文件中文字和圖片的metadata。請注意，以下的程式區塊可能需要幾分鐘才能完成執行。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFgRwzokrP7v"
      },
      "source": [
        "注意事項：\n",
        "\n",
        "目前的實現最適用於以下情況：\n",
        "\n",
        "* 文件中包含文字和圖片的組合。\n",
        "* 文件中的表格以圖片形式呈現。\n",
        "* 文件中的圖片不需要太多上下文信息。</br>\n",
        "(但為了方便，本範例檔案僅含有文字)\n",
        "\n",
        "另外，\n",
        "* 你也可以使用常規的 RAG 方法。可以參考這份檔案 [RAG_text_only](https://github.com/kevin6449/LANGCHAIN_RAG/blob/main/LangChain_RAG.ipynb)\n",
        "* 如果文件包含額外特定領域的知識，可以將這些信息傳遞至下方的提示語中。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nflT_j-9QzC_"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ 不要傳送超過50頁的資料，你可能會遇到流量限制 ⚠️</b></br>\n",
        "⚠️ 如果你遇到了其他形式的流量限制，請開啟下方的add_sleep_after_page並設定sleep_time_after_page ⚠️\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8hE0tWD-lf8",
        "outputId": "e335c507-9685-4c7d-f44e-3fff02baa089"
      },
      "outputs": [],
      "source": [
        "pdf_folder_path = \"pdf/\"\n",
        "\n",
        "\n",
        "image_description_prompt = \"\"\"Explain what is going on in the image.\n",
        "If it's a table, extract all elements of the table.\n",
        "If it's a graph, explain the findings in the graph.\n",
        "Do not include any numbers that are not mentioned in the image.\n",
        "\"\"\"\n",
        "\n",
        "# Extract text and image metadata from the PDF document\n",
        "text_metadata_df, image_metadata_df = get_document_metadata(\n",
        "    multimodal_model,  # we are passing Gemini 1.5 Pro model\n",
        "    pdf_folder_path,\n",
        "    image_save_dir=\"images\",\n",
        "    image_description_prompt=image_description_prompt,\n",
        "    embedding_size=2048,\n",
        "    add_sleep_after_page = True, # Uncomment this if you are running into API quota issues\n",
        "    sleep_time_after_page = 35,\n",
        "    # generation_config = # see next cell\n",
        "    # safety_settings =  # see next cell\n",
        ")\n",
        "\n",
        "print(\"\\n\\n --- Completed processing. ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miBBoEXwh2rN"
      },
      "source": [
        "#### 檢視處理過的文字metadata\n",
        "以下的程式區塊將產生一個metadata表格，描述不同部分的文字metadata，包括：\n",
        "\n",
        "- text: 來自頁面的原始文字\n",
        "- text_embedding_page: 頁面上原始文字的嵌入向量\n",
        "- chunk_text: 將原始文字分割成較小的區塊\n",
        "- chunk_number: 每個文字區塊的索引\n",
        "- text_embedding_chunk: 每個文字區塊的嵌入向量\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "collapsed": true,
        "id": "6t3AIGFar8Mo",
        "outputId": "1101cf2d-02ba-4377-c8a7-5b6d3b79b9bb"
      },
      "outputs": [],
      "source": [
        "text_metadata_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBhoOkutUtPr"
      },
      "source": [
        "### 匯入輔助函數以實現 RAG\n",
        "你將匯入以下函數，這些函數將在本筆記本的其餘部分中用來實現 RAG：\n",
        "\n",
        "- ```get_similar_text_from_query()```：根據文字查詢，使用餘弦相似度算法從文件中找出相關的文字。此函數使用中繼資料中的文字嵌入向量來計算結果，並可以根據最高分數、頁碼/區塊號或嵌入向量大小來過濾結果。\n",
        "- ```print_text_to_text_citation()```：印出從 ```get_similar_text_from_query()``` 函數檢索到的文字來源（引用）和細節。\n",
        "- ```get_similar_image_from_query()```：根據圖片路徑或圖片，從文件中找出相關的圖片。此函數使用中繼資料中的圖片嵌入向量。\n",
        "- ```print_text_to_image_citation()```：印出從 ```get_similar_image_from_query()``` 函數檢索到的圖片來源（引用）和細節。\n",
        "- ```get_gemini_response()```：與 Gemini 模型互動，基於文字和圖片輸入的結合來回答問題。\n",
        "- ```display_images()```：顯示一系列圖片，這些圖片可以是路徑或 PIL 圖片對象。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tngn_vrIKdE1"
      },
      "outputs": [],
      "source": [
        "from intro_multimodal_rag_utils import (\n",
        "    display_images,\n",
        "    get_gemini_response,\n",
        "    get_similar_image_from_query,\n",
        "    get_similar_text_from_query,\n",
        "    print_text_to_image_citation,\n",
        "    print_text_to_text_citation,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHuLlEvSKFWt"
      },
      "source": [
        "#### 文字搜尋\n",
        "讓我們從一個簡單的問題開始，看看使用文字嵌入向量的簡單文字搜尋是否能夠回答這個問題。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mrFVhtCut7t"
      },
      "outputs": [],
      "source": [
        "query = \"what is the name of the main characte? Where do you get this conclusion?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWw7-AIar-S8"
      },
      "source": [
        "### 使用文字搜尋來尋找相關資料"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEzP6Yyv7N-G",
        "outputId": "9e6a106f-3598-49d9-bb41-921666a4089b"
      },
      "outputs": [],
      "source": [
        "# Matching user text query with \"chunk_embedding\" to find relevant chunks.\n",
        "matching_results_text = get_similar_text_from_query(\n",
        "    query,\n",
        "    text_metadata_df,\n",
        "    column_name=\"text_embedding_chunk\",\n",
        "    top_n=7,\n",
        "    chunk_text=True,\n",
        ")\n",
        "\n",
        "# Print the matched text citations\n",
        "print_text_to_text_citation(matching_results_text, print_top=False, chunk_text=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0bnOOf2rP70"
      },
      "source": [
        "我們可以發現，第一個高分結果包含了我們需要的信息，但仔細檢查後發現，它提到該信息存在於“以下”表格中。該表格數據是以圖片形式存在，而非文字，因此除非能處理圖片及其數據，否則很可能會錯過這些信息。\n",
        "\n",
        "不過，讓我們將相關的文字區塊輸入 Gemini 1.0 Pro 模型，看看它是否能在考慮文件中所有區塊的情況下給出我們想要的答案。這就像是基本的文字型 RAG 實現。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQREZrGsXmPc",
        "outputId": "8cccf54a-7b07-41e8-f007-05d03bcd27a8"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"\\n **** Result: ***** \\n\")\n",
        "\n",
        "# All relevant text chunk found across documents based on user query\n",
        "context = \"\\n\".join(\n",
        "    [value[\"chunk_text\"] for key, value in matching_results_text.items()]\n",
        ")\n",
        "\n",
        "instruction = f\"\"\"Answer the question with the given context.\n",
        "If the information is not available in the context, return \"not available in the context\" and explain the situation.\n",
        "Question: {query}\n",
        "Context: {context}\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "# Prepare the model input\n",
        "model_input = instruction\n",
        "\n",
        "# Generate Gemini response with streaming output\n",
        "print(get_gemini_response(\n",
        "    text_model,  # we are passing Gemini 1.0 Pro\n",
        "    model_input=model_input,\n",
        "    stream=True,\n",
        "    generation_config=GenerationConfig(temperature=0.2),\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4oxQO3bfKEs"
      },
      "source": [
        "接下來，讓我們來問問更多關於主角的故事吧~~"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "o-GbnrEgXmr6"
      },
      "outputs": [],
      "source": [
        "while(True):\n",
        "  query = input()\n",
        "\n",
        "  # Matching user text query with \"chunk_embedding\" to find relevant chunks.\n",
        "  matching_results_text = get_similar_text_from_query(\n",
        "      query,\n",
        "      text_metadata_df,\n",
        "      column_name=\"text_embedding_chunk\",\n",
        "      top_n=7,\n",
        "      chunk_text=True,\n",
        "  )\n",
        "\n",
        "  # Print the matched text citations\n",
        "  #print_text_to_text_citation(matching_results_text, print_top=False, chunk_text=True)\n",
        "\n",
        "\n",
        "  print(\"\\n **** Result: ***** \\n\")\n",
        "\n",
        "  # All relevant text chunk found across documents based on user query\n",
        "  context = \"\\n\".join(\n",
        "      [value[\"chunk_text\"] for key, value in matching_results_text.items()]\n",
        "  )\n",
        "\n",
        "  instruction = f\"\"\"Answer the question with the given context.\n",
        "  If the information is not available in the context, return \"not available in the context\" and explain the situation.\n",
        "  Question: {query}\n",
        "  Context: {context}\n",
        "  Answer:\n",
        "  \"\"\"\n",
        "\n",
        "  # Prepare the model input\n",
        "  model_input = instruction\n",
        "\n",
        "  # Generate Gemini response with streaming output\n",
        "  print(get_gemini_response(\n",
        "      text_model,  # we are passing Gemini 1.0 Pro\n",
        "      model_input=model_input,\n",
        "      stream=True,\n",
        "      generation_config=GenerationConfig(temperature=0.2),\n",
        "  ))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
